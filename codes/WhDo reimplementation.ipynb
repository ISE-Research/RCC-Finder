{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function compute interval between two time points\n",
    "def time_interval(firstdate, seconddate):\n",
    "    firstday = int(firstdate[8:10])\n",
    "    firstmonth = int(firstdate[5:7])\n",
    "    firstyear = int(firstdate[2:4])\n",
    "    secondday = int(seconddate[8:10])\n",
    "    secondmonth = int(seconddate[5:7])\n",
    "    secondyear = int(seconddate[2:4]) \n",
    "    first = firstyear * 360 + (firstmonth - 1) * 30 + firstday\n",
    "    second = secondyear * 360 + (secondmonth - 1) * 30 + secondday\n",
    "    days = second - first\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate whodo file_change score\n",
    "import pandas as pd\n",
    "import math\n",
    "address = \"results/flink/file_change_num.csv\"\n",
    "numresult_df = pd.read_csv(address)\n",
    "address = \"results/flink/file_change_num.csv\"\n",
    "timeresult_df = pd.read_csv(address)\n",
    "address = \"results/flink/file_change_num.csv\"\n",
    "expresult_df = pd.read_csv(address)\n",
    "    \n",
    "address = \"features/flink/datedPullFile.csv\"\n",
    "file_df = pd.read_csv(address)\n",
    "address = \"features/flink/datedCommitFileAuthor.csv\"\n",
    "commit_df = pd.read_csv(address)\n",
    "for i in range(numresult_df.shape[0]):\n",
    "    number = numresult_df.loc[i, 'number']\n",
    "    print(i)\n",
    "    print(number)\n",
    "    myfile_df = file_df[file_df.pull_number == number]\n",
    "    if myfile_df.shape[0] == 0:\n",
    "        continue\n",
    "    date = myfile_df.iloc[0, 2]\n",
    "    for j in range(myfile_df.shape[0]):\n",
    "        mycommit_df = commit_df[commit_df.date < date]\n",
    "        path = myfile_df.iloc[j, 1]\n",
    "        newcommit_df = mycommit_df[mycommit_df.file_path==path]\n",
    "        num_df = newcommit_df.groupby(['author_login'])['oid'].count()\n",
    "        time_df = newcommit_df.groupby(['author_login'])['date'].max()\n",
    "        for k in range(num_df.shape[0]):\n",
    "            committer = num_df.index[k]\n",
    "            num = num_df.iloc[k]\n",
    "            max_value = time_df.iloc[k]\n",
    "            numresult_df.loc[i, committer] += num\n",
    "            lasttime = time_interval(max_value, date)\n",
    "            if lasttime !=0:\n",
    "                timeresult_df.loc[i, committer] += num / lasttime\n",
    "            else:\n",
    "                timeresult_df.loc[i, committer] += num\n",
    "            expresult_df.loc[i, committer] += num / math.exp(lasttime/7)\n",
    "\n",
    "numresult_df.to_csv(\"results/flink/file_change_num.csv\", index = False)\n",
    "timeresult_df.to_csv(\"results/flink/file_change_time.csv\", index = False)\n",
    "expresult_df.to_csv(\"results/flink/file_change_exp.csv\", index = False)\n",
    "timeresult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate whodo pach_change score\n",
    "import pandas as pd\n",
    "import math\n",
    "address = \"results/beam/pack_change_num.csv\"\n",
    "timeresult_df = pd.read_csv(address)\n",
    "address = \"features/beam/datedPullFile.csv\"\n",
    "file_df = pd.read_csv(address)\n",
    "address = \"features/beam/datedCommitFileAuthor.csv\"\n",
    "commit_df = pd.read_csv(address)\n",
    "def function_r(x):\n",
    "    A = x.split(\"/\")\n",
    "    del A[-1]\n",
    "    return('/'.join(A))\n",
    "file_df['file_path'] = file_df['file_path'].apply(function_r)\n",
    "for i in range(numresult_df.shape[0]):\n",
    "    number = numresult_df.loc[i, 'number']\n",
    "    print(i)\n",
    "    print(number)\n",
    "    myfile_df = file_df[file_df.pull_number == number]\n",
    "    myfile_df = myfile_df.drop_duplicates(subset=['file_path'], keep=\"first\")\n",
    "    if myfile_df.shape[0] == 0:\n",
    "        continue\n",
    "    date = myfile_df.iloc[0, 2]\n",
    "    for j in range(myfile_df.shape[0]):\n",
    "        mycommit_df = commit_df[commit_df.date < date]\n",
    "        path = myfile_df.iloc[j, 1]\n",
    "        newcommit_df = mycommit_df[mycommit_df.file_path.str.startswith(path)]\n",
    "        newcommit_df = newcommit_df.drop_duplicates(subset=['oid', 'author_login'], keep=\"last\")\n",
    "        num_df = newcommit_df.groupby(['author_login'])['oid'].count()\n",
    "        time_df = newcommit_df.groupby(['author_login'])['date'].max()\n",
    "        for k in range(num_df.shape[0]):\n",
    "            committer = num_df.index[k]\n",
    "            num = num_df.iloc[k]\n",
    "            max_value = time_df.iloc[k]\n",
    "            lasttime = time_interval(max_value, date)\n",
    "            if lasttime !=0:\n",
    "                timeresult_df.loc[i, committer] += num / lasttime\n",
    "            else:\n",
    "                timeresult_df.loc[i, committer] += num\n",
    "timeresult_df.to_csv(\"results/beam/pack_change_time.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate whodo file_review score\n",
    "import pandas as pd\n",
    "import math\n",
    "address = \"results/zookeeper/file_review_num.csv\"\n",
    "numresult_df = pd.read_csv(address)\n",
    "address = \"results/zookeeper/file_review_num.csv\"\n",
    "timeresult_df = pd.read_csv(address)\n",
    "address = \"results/zookeeper/file_review_num.csv\"\n",
    "expresult_df = pd.read_csv(address)\n",
    "address = \"features/spark/datedPullFile.csv\"\n",
    "file_df = pd.read_csv(address)\n",
    "address = \"features/spark/datedReviewFile.csv\"\n",
    "review_df = pd.read_csv(address)\n",
    "\n",
    "for i in range(numresult_df.shape[0]):\n",
    "    number = numresult_df.loc[i, 'number']\n",
    "    print(i)\n",
    "    print(number)\n",
    "    myfile_df = file_df[file_df.pull_number == number]\n",
    "    if myfile_df.shape[0] == 0:\n",
    "        continue\n",
    "    date = myfile_df.iloc[0, 2]\n",
    "    for j in range(myfile_df.shape[0]):\n",
    "        myreview_df = review_df[review_df.date < date]\n",
    "        path = myfile_df.iloc[j, 1]\n",
    "        newreview_df = myreview_df[myreview_df.file_path == path]\n",
    "        newreview_df = newreview_df.drop_duplicates(subset=['pull_number', 'reviewer_login'], keep=\"last\")\n",
    "        num_df = newreview_df.groupby(['reviewer_login'])['pull_number'].count()\n",
    "        time_df = newreview_df.groupby(['reviewer_login'])['date'].max()\n",
    "        for k in range(num_df.shape[0]):\n",
    "            reviewer = num_df.index[k]\n",
    "            num = num_df.iloc[k]\n",
    "            max_value = time_df.iloc[k]\n",
    "            numresult_df.loc[i, reviewer] += num\n",
    "            lasttime = time_interval(max_value, date)\n",
    "            if lasttime !=0:\n",
    "                timeresult_df.loc[i, reviewer] += num / lasttime\n",
    "            else:\n",
    "                timeresult_df.loc[i, reviewer] += num\n",
    "            expresult_df.loc[i, reviewer] += num / math.exp(lasttime/7)\n",
    "\n",
    "numresult_df.to_csv(\"results/spark/file_review_num.csv\", index = False)\n",
    "timeresult_df.to_csv(\"results/spark/file_review_time.csv\", index = False)\n",
    "expresult_df.to_csv(\"results/spark/file_review_exp.csv\", index = False)\n",
    "timeresult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc70935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate whodo pack_review score\n",
    "import pandas as pd\n",
    "import math\n",
    "address = \"results/spark/pack_review_num.csv\"\n",
    "timeresult_df = pd.read_csv(address)\n",
    "address = \"features/spark/datedPullFile.csv\"\n",
    "file_df = pd.read_csv(address)\n",
    "address = \"features/spark/datedReviewFile.csv\"\n",
    "review_df = pd.read_csv(address)\n",
    "def function_r(x):\n",
    "    A = x.split(\"/\")\n",
    "    del A[-1]\n",
    "    return('/'.join(A))\n",
    "file_df['file_path'] = file_df['file_path'].apply(function_r)\n",
    "for i in range(numresult_df.shape[0]):\n",
    "    number = numresult_df.loc[i, 'number']\n",
    "    print(i)\n",
    "    print(number)\n",
    "    myfile_df = file_df[file_df.pull_number == number]\n",
    "    myfile_df = myfile_df.drop_duplicates(subset=['file_path'], keep=\"first\")\n",
    "    if myfile_df.shape[0] == 0:\n",
    "        continue\n",
    "    date = myfile_df.iloc[0, 2]\n",
    "    for j in range(myfile_df.shape[0]):\n",
    "        myreview_df = review_df[review_df.date < date]\n",
    "        path = myfile_df.iloc[j, 1]\n",
    "        newreview_df = myreview_df[myreview_df.file_path.str.startswith(path)]\n",
    "        newreview_df = newreview_df.drop_duplicates(subset=['pull_number', 'reviewer_login'], keep=\"last\")\n",
    "        num_df = newreview_df.groupby(['reviewer_login'])['pull_number'].count()\n",
    "        time_df = newreview_df.groupby(['reviewer_login'])['date'].max()\n",
    "        for k in range(num_df.shape[0]):\n",
    "            reviewer = num_df.index[k]\n",
    "            num = num_df.iloc[k]\n",
    "            max_value = time_df.iloc[k]\n",
    "            lasttime = time_interval(max_value, date)\n",
    "            if lasttime !=0:\n",
    "                timeresult_df.loc[i, reviewer] += num / lasttime\n",
    "            else:\n",
    "                timeresult_df.loc[i, reviewer] += num\n",
    "timeresult_df.to_csv(\"results/spark/pack_review_time.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4640d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate final whodo_scores\n",
    "import pandas as pd\n",
    "address = \"results/spark/file_change_time.csv\"\n",
    "filechange_df = pd.read_csv(address)\n",
    "filechange_df = filechange_df.iloc[:, 1:]\n",
    "address = \"results/spark/file_review_time.csv\"\n",
    "filereview_df = pd.read_csv(address)\n",
    "filereview_df = filereview_df.iloc[:, 1:]\n",
    "address = \"results/spark/pack_change_time.csv\"\n",
    "packchange_df = pd.read_csv(address)\n",
    "packchange_df = packchange_df.iloc[:, 1:]\n",
    "address = \"results/spark/pack_review_time.csv\"\n",
    "packreview_df = pd.read_csv(address)\n",
    "packreview_df = packreview_df.iloc[:, 1:]\n",
    "ans_df = filechange_df + filereview_df + packchange_df + packreview_df\n",
    "ans_df.to_csv(\"results/spark/whodo_scores.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
